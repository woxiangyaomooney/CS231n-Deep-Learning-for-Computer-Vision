# 训练神经网络

![image-20231014213632594](./图片/image-20231014213632594.png)

Activation Funcations激活函数

![image-20231014213737818](./图片/image-20231014213737818.png)

sigmoid函数缺点：梯度消失、值恒大于零不够灵活（不满足零中心）、含有指数项计算量大

- 饱和神经元使梯度消失：当输入值特别小或大时，梯度值就无限接近于零，由链式法则反向传播，零梯度就会传递到下游的节点，即梯度就会消失

- 关于w的梯度是局部梯度(x)乘上L回传的梯度，局部梯度恒正，L回传的梯度有正有负，则w的梯度要不全为正，要不全为负，这样


​	即所有的w的梯度只能同时为正或为负

![image-20231111173250594](./图片/image-20231111173250594.png)

![image-20231111171211635](./图片/image-20231111171211635.png)

​	当进行梯度更新的时候，若L值为负，则w的梯度为负，若L值为正，即w的梯度为正；

​	这样梯度下降的方向只能是沿第一象限或第三象限，所以w更新的效率太低

- 指数计算的代价会稍微高一点（不是主要问题，卷积和点积的计算代价更大）

  ![image-20231111171813278](./图片/image-20231111171813278.png)

其他激活函数：

![image-20231111172318913](./图片/image-20231111172318913.png)

- 满足零中心（zero-centered），有梯度消失

![image-20231111173355229](./图片/image-20231111173355229.png)

ReLu优点：

- 右半部分没有梯度消失/梯度饱和
- 收敛速度更快

缺点：

- 不满足零中心
- 左边恒为零，梯度饱和，那么如果初始W设置得不大好，导致输出全是小于0的，这样激活函数的输出值全是0，反向求导的时候也全是0，不会进行梯度更新

![image-20231111174038030](./图片/image-20231111174038030.png)

对于不同的权重对应不同的分割超平面，数据经过加权后输入RELU，通过超平面将空间分割为0和非0区域，不同的RELU训练得到的分割超平面也相应不同

当数据云（即所有输入样本构成的集合）全部处于某个神经元对应直线的下方时，将导致无论输入哪个样本都无法使神经元的输出为正，因此该神经元在训练过程中将一直无法更新，所以说数据dead了



所以人们倾向于使用较小的权重来初始化ReLu，偏差值大多时候初始化为0，来增加它在初始化时被激活的可能性

改进的ReLu函数：

![image-20231111174615038](./图片/image-20231111174615038.png)

![image-20231111174701430](./图片/image-20231111174701430.png)

  

![image-20231111174855778](./图片/image-20231111174855778-1699696136390-1.png)

总结：

激活函数的目的是制造非线性差异，但也要兼顾计算量、导数大小及连续性、神经元死亡等问题

![image-20231111174957237](./图片/image-20231111174957237.png)

通常使用ReLu，可尝试使用其他函数，不会使用sigmoid

## Data Preprocessiong预处理

数据预处理：

![image-20231112165108086](./图片/image-20231112165108086.png)

 归一化

零均值化

原因：

![image-20231112165432892](./图片/image-20231112165432892.png)

在归一化  之前，分类器对数据的微小扰动特别敏感，所以网络权重矩阵的微小摄动。就会造成该层输出的巨大摄动

对训练集和测试集的数据做同样的预处理

权重预处理：

当所有权重值都为0时，相当于所有神经元都相同

初始化权重过小的时候，更新缓慢，网络崩溃

过大的时候，梯度饱和

 解决办法：使用Xavier初始化



## 批量归一化Batch Normalization

比较清晰的解释：[博文](https://blog.csdn.net/Yasin0/article/details/93379629)

 对输入数据进行标准化处理的原因？

使输入数据各个特征的分布相近：

神经网络学习的本质就是学习数据的分布，如果训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；
在使用小批量数据对神经网络进行训练时，若每批训练数据的分布各不相同，网络在每次迭代都去学习适应不同的分布，这会大大降低网络的训练速度；
为什么要使用批量归一化？

使用浅层模型时，随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。对深层神经网络来说，随着网络训练的进行，前一层参数的调整使得后一层输入数据的分布发生变化，各层在训练的过程中就需要不断的改变以适应学习这种新的数据分布。所以即使输入数据已做标准化，训练中模型参数的更新依然很容易导致后面层输入数据分布的变化，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。最终造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。如果训练过程中，训练数据的分布一直在发生变化，那么将不仅会增大训练的复杂度，影响网络的训练速度而且增加了过拟合的风险。

在模型训练时，在应用激活函数之前，先对一个层的输出进行归一化，将所有批数据强制在统一的数据分布下，然后再将其输入到下一层，使整个神经网络在各层的中间输出的数值更稳定。从而使深层神经网络更容易收敛而且降低模型过拟合的风险。

批量归一化的优势：

不加批量归一化的网络需要慢慢的调整学习率时，网络中加入批量归一化时，可以采用初始化很大的学习率，然后学习率衰减速度也很大，因此这个算法收敛很快。
BN可以大大提高模型训练速度，提高网络泛化性能。
数据批量归一化后相当于只使用了S型激活函数的线性部分，可以缓解S型激活函数反向传播中的梯度消失的问题。
深层神经网络在做非线性变换前的激活输入值，随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，整体分布逐渐往非线性函数的取值区间的上下限两端靠近，这会导致反向传播时低层神经网络的梯度消失，BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的线性区域，其对应的导数远离导数饱和区 ，这样输入的小变化就会导致损失函数较大的变化，避免梯度消失问题产生，学习收敛速度快，能大大加快训练速度。

全连接层如何使用批量归一化？

将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为u，权重参数和偏差参数分别为W和b，激活函数为ϕ，批量归一化的运算符为BN。那么，使用批量归一化的全连接层输出为

![image-20231112165014028](./图片/image-20231112165014028.png)

## 超参数搜索hyperparameter search：

![image-20231112165842875](./图片/image-20231112165842875.png)

### **· 试错法（Babysitting）**

纯手工操作，设计了一个实验后，遵循学习过程的所有步骤（从数据收集到特征图映射的可视化），然后在超参数上依次迭代直到时间终止。

### **· 网格搜索（Grid Search）**

网格搜索，是一种简单尝试所有可能配置的方法。
它的工作流程是这样的：
**·** 定义一个N维的网格，其中每一个映射都代表一个超参数，例如，n＝（learning_rate, dropout_rate, batch_size）
**·** 对于每个维度，定义可能的取值范围: 例如，batch_size = [4, 8, 16, 32, 64, 128, 256]
**·** 搜索所有可能的配置，等待结果以建立最佳配置: 例如，*C1* = (0.1, 0.3, 4) ->acc = 92%, *C2* = (0.1, 0.35, 4) ->acc = 92.3%···
下图说明了一个简单的二维网格搜索的Dropout Rate和Learning Rate。

![动图封面](https://pic4.zhimg.com/v2-89056975dc697cae8f59fcf7ece316fb_b.jpg)

![img](https://pic2.zhimg.com/80/v2-b310ce93dfcd614e16a37aa0f1731419_1440w.webp)

并行执行两个变量的网格搜索


这种策略没有考虑到计算背景，但这意味着可用的计算资源越多，那么同时可以尝试的猜测就会越多。它的痛点被称为维度灾难，意思是我们增加的维度越多，搜索就会变得越困难，最终导致策略失败。

### **· 随机搜索（Random Search）**

网格搜索和随机搜索之间唯一的区别在于策略周期的第一步：随机搜索在配置空间上随机选择点。

![动图](https://pic3.zhimg.com/v2-7592374039cc996ed2c6aecb072d84ae_b.webp)

![img](https://pic4.zhimg.com/80/v2-3c12238d2335f517c5b83d904fab4673_1440w.webp)

网格搜索vs随机搜索


通过在两个超参数空间上搜索最佳配置来对比这两种方法，并假定一个参数比另一个参数更重要。深度学习模型，如前面所说，实际上包含许多超参数，通常研究者知道哪些对训练影响最大。
在网格搜索中，即使我们已经训练了9个模型，但给每个变量只使用了3个值，然而，在随机搜索中，多次选择相同变量的可能性微乎其微。如果用第二种方法，那么就会给每个变量使用9个不同的值来训练9个模型。
上图中，从每个布局顶部的空间搜索可以看出，使用随机搜索更广泛地研究了超参数空间，这将帮助我们在较少的迭代中找到最佳配置。
总之，如果搜索空间包含3到4个维度，则不要使用网格搜索。相反，使用随机搜索，则会为每个搜索任务提供了一个非常好的基线。

![img](https://pic3.zhimg.com/80/v2-5c92a47eef735ffc1ad228982b077886_1440w.webp)

​                                                                                              网格搜索与随机搜索的利弊

不幸的是，网格搜索和随机搜索有一个共同的缺点：“每个新的猜测都独立于之前的运行！”
相比之下，Babysitting的优势就显现出来了。Babysitting之所以有效，是因为研究者有能力利用过去的猜测，将其作为改进下一步工作的资源，来有效地推动搜索和实验。

### **· 贝叶斯优化（Bayesian Optimization）**



贝叶斯策略建立了一个代理模型，试图从超参数配置中预测我们所关注的度量指标。在每一次的迭代中，我们对代理会变得越来越有信心，新的猜测会带来新的改进，就像其它搜索策略一样，它也会等到耗尽资源的时候终止。

![img](https://pic4.zhimg.com/80/v2-feb5bb3907b58f835c097e3548188c2b_1440w.webp)

​                                                                                  贝叶斯优化工作流程

使用先前对损失**f**的观察结果，以确定下一个(最优)点来抽样**f**。该算法大致可以概括如下。

1. 使用先前评估的点X1*:n*，计算损失f的后验期望。
2. 在新的点X的抽样损失f，从而最大化f的期望的某些方法。该方法指定f域的哪些区域最适于抽样。

重复这些步骤，直到满足某些收敛准则。

![动图](https://pic1.zhimg.com/v2-7dc04de849257291bfadbe68f0222928_b.webp)

- 交叉验证策略

![image-20231111195626395](./图片/image-20231111195626395.png)

- ### 粗细粒交叉搜索（coarse to fine search）

进行超参数优化时，一开始可能会处理很大的搜索范围，几次迭代后就可以缩小参数范围，圈定合适的超参数所在的范围，然后再对这个小范围重复过程

初始范围应该足够宽到网络不会超过范围的任一边



## Fancier optimization

### SGD随机梯度下降算法的问题：

- #### 方向分配不均

![image-20231112174337234](./图片/image-20231112174337234.png)

- #### 存在局部极小值和鞍点

![image-20231112174647473](./图片/image-20231112174647473.png)

高维度计算时出现鞍点，意味着某些方向上损失会增加，某些方向损失会减少，

高维度鞍点更常见，而出现局部极小值（全部方向损失都会增加）的情况反而更少遇到



### 改进算法：

#### Momentum

加入惯性/速度，使得优化不容易落入局部最优解或者停留在鞍点，可以减少常规的带有噪声的梯度下降SGD的曲折，也有利于处理高条件数的问题

例如具有惯性的小球，运动时会越过鞍点和局部最小值

![image-20231119081903147](./图片/image-20231119081903147.png)

#### Nesterov Momentum

在普通的SGD动量中，估算当前位置的梯度，取梯度和当前速度的混合作为实际前进的方向

在Nesterov Momentum中是速度过去后测得的梯度，然后返回来把速度加上那边的梯度再实际步进

![image-20231119082709390](./图片/image-20231119082709390.png)

![image-20231119083237927](./图片/image-20231119083237927.png)

Nesterov向量同时包含了当前速度向量和先前速度向量的误差修正

####  AdaGrad:

累加梯度平方和，使得步长有了一个很好的性质，在梯度下降很慢的维度上的训练速度会加快，在梯度下降很快的维度上的训练速度减慢

但是在复杂条件中会在局部最小值点停滞不前

![image-20231119084309732](./图片/image-20231119084309732.png)

#### RMSProp：

给累加的平方梯度和按照一定比率下降，和动量优化法类似

RMSProp会逐渐遗忘掉过去的梯度，在做加法运算时将新梯度的信息更多的反映出来

可以解决AdaGred在复杂条件中会在局部最小值点停滞不前的问题，但是由于给了梯度和一个衰减的比率，训练有可能  总是一直在变慢

（分母上+1e7-10为了使分母不为零

![image-20231119084802287](./图片/image-20231119084802287.png)

#### Adam：

  动量Momentum和平方梯度和RMSProp算法的混合

一般情况下的首选

![image-20231119085626908](./图片/image-20231119085626908.png)

### 学习率的选择：

#### 步长衰减，指数衰减

一开始将学习率设置为较大的数值

步长衰减：训练了多少步后对学习率进行衰减

指数衰减：在训练过程中持续衰减

![image-20231119090009866](./图片/image-20231119090009866.png)

步长衰减：

![image-20231119090209114](./图片/image-20231119090209114.png)

学习率衰减在SGD+Momentum中更常用，在Adam中不常用

设置学习率的方法应是，先尝试不用衰减，观察损失曲线，找出想衰减的地方，再使用衰减



#### 二阶优化：

##### 牛顿法

求hessian矩阵，但对深度学习来说不太实际

![image-20231119090742195](./图片/image-20231119090742195.png)

##### L-BGFGS二阶优化器

![image-20231119090920363](./图片/image-20231119090920363.png)

![image-20231119091019858](./图片/image-20231119091019858.png)

Adam和L-BFGS适用情况：

![image-20231119091055133](./图片/image-20231119091055133.png)

### 减少训练和测试之间的误差差距（减少过拟合）：

#### 模型集成/集成学习

![image-20231119091312769](./图片/image-20231119091312769.png)

更高级一点，保留学习过程中的模型快照，进行集成

![image-20231119091454652](./图片/image-20231119091454652.png)

图中使用的方法是开始时将学习率设置为一个非常小的数，然后突然变大，再突然变小......



#### Polyak平均：

在训练模型时对不同时刻的每个模型参数，求指数衰减平均值，从而得到网络训练中一个比较平滑的集成模型，之后使用这些平滑衰减平均后的模型参数，而不是截至在某一时刻的模型参数

![image-20231119091718061](./图片/image-20231119091718061.png)

## Regularization

### 正则化：减少过拟合

![image-20231119093326455](./图片/image-20231119093326455.png)

### Dropout：

- 每次在网络中正向传递时，在每一层随机将部分神经元置零
- 一般在全连接层使用
- 抑制过拟合的解释：
  - 减少了特征间的相互作用
  - 相当于集合学习，每次丢的结点都不同，相当于每次训练都是在训练新模型，测试时数据通过每个结点相当于把所有集合都给用了一遍

![image-20231119093247868](./图片/image-20231119093247868.png)

![image-20231119093602312](./图片/image-20231119093602312.png)

### Dropout的改进:

![image-20231119093940701](./图片/image-20231119093940701.png)

![image-20231119094949328](./图片/image-20231119094949328.png)

![image-20231119095025822](./图片/image-20231119095025822.png)

![image-20231119095042510](./图片/image-20231119095042510.png)



dropout和Batch Normalization都是常见的正则化方法，在训练过程中引入随机性减少过拟合，在测试时又抵消掉随机性

![image-20231119095255269](./图片/image-20231119095255269.png)

### 数据增强：

在不更改标签的情况下对数据进行改动

- 翻转，旋转
- 裁剪
- 色彩抖动

![image-20231119095401003](./图片/image-20231119095401003.png)

![image-20231119095422087](./图片/image-20231119095422087.png)

![image-20231119095511076](./图片/image-20231119095511076.png)

### 总结：

![image-20231119095837577](./图片/image-20231119095837577.png)

Stochastic Depth(上图)

Fractional Max Pooling：

![image-20231119095923406](./图片/image-20231119095923406.png)

DropConnect:

![image-20231119095949720](./图片/image-20231119095949720.png)

大多数情况下使用Batch Normalization就足够

## Transfer Learning迁移学习

迁移学习能够不需要大的数据集也能训练卷积神经网络

当差异不太大的时候，可以使用原有模型的参数进行微调，当差异比较大的时候就需要初始化大部分参数重新进行训练

在解决实际问题时很好用，当没有大数据集的时候，可以从网络上下载已经调试好的模型进行微调使用

![image-20231119102228100](./图片/image-20231119102228100.png)

![image-20231119102330662](./图片/image-20231119102330662.png)

# 总结

![image-20231119102453374](./图片/image-20231119102453374.png)
