# 训练神经网络

![image-20231014213632594](./图片/image-20231014213632594.png)

Activation Funcations激活函数

![image-20231014213737818](./图片/image-20231014213737818.png)

sigmoid函数缺点：

- 饱和神经元使梯度消失：当输入值特别小或大时，梯度值就无限接近于零，由链式法则反向传播，零梯度就会传递到下游的节点，即梯度就会消失

关于W_i的梯度是L关于f、f关于z、以及x三项的乘积，x是上一sigmoid 输出，输出恒为正，所以梯度就恒为正 

他的意思是所有的w的梯度只能同时为正或未负，这样w更新的效率太低 